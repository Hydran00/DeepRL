{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCALETTA\n",
    "  ### 1. Import delle librerie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUZIONE AL PROGETTO (== abstract)\n",
    " 1) Motivazioni\n",
    " 2) Spiegazione degli approcci utilizzati\n",
    " 3) Accenno ai risultati ottenuti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insipration for the project came from this [paper](https://arxiv.org/pdf/1703.07579.pdf) from 2017, using some unupdated approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time as tm\n",
    "from collections import deque\n",
    "import gdown\n",
    "import tarfile\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from functools import total_ordering\n",
    "from PIL import Image #, ImageDraw\n",
    "\n",
    "import clip\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler, SequentialSampler\n",
    "from torch.distributions import Categorical\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# TODO: VERIFICARE CORRETTO FUNZIONAMENTO!!!!\n",
    "import argparse\n",
    "import wandb\n",
    "# fro env.env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "HISTORY_LENGHT = 12\n",
    "ACT_NUM = 9\n",
    "INPUT_SIZE = 1024+1024+5+ACT_NUM*HISTORY_LENGTH\n",
    "BBOX_EMB=1024\n",
    "NUM_SEGMENTS=9\n",
    "\n",
    "\n",
    "VALIDATION_SET_SIZE=2572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x.1 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x.2 Dataset and Dataloader (TODO: update dataset to work with dataloader)\n",
    "\n",
    "Our dataset is **RefCOCOg**, a dataset for referring expression comprehension in the context of visual grounding, based on the **COCO** dataset. We created a custom *PyTorch* class to deal with it, downloading and/or extracting it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCOCOg(Dataset):\n",
    "    FILE_ID = \"1wyyksgdLwnRMC9pQ-vjJnNUn47nWhyMD\"\n",
    "    ARCHIVE_NAME = \"refcocog.tar.gz\"\n",
    "    NAME = \"refcocog\"\n",
    "    ANNOTATIONS = \"annotations/refs(umd).p\"\n",
    "    JSON = \"annotations/instances.json\"\n",
    "    IMAGES = \"images\"\n",
    "    IMAGE_NAME = \"COCO_train2014_{}.jpg\"\n",
    "    FUSION_NET_PATH = \"fusion.pt\"\n",
    "\n",
    "\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self._check_dataset()\n",
    "        self._filter_annotation(\n",
    "            os.path.join(self.data_dir, self.NAME, self.ANNOTATIONS)\n",
    "        )\n",
    "        self._load_json()\n",
    "        self.transform = transform\n",
    "        self.model, self.preprocess = clip.load(\"RN50\", device=DEVICE)#,jit=False)\n",
    "        # ret = torch.load(\"fusion.pt\",map_location=DEVICE)\n",
    "        # self.fusion_embedding_net = FusionEmbedding()\n",
    "        # self.fusion_embedding_net.load_state_dict(torch.load(RefCOCOg.FUSION_NET_PATH)[\"model\"])\n",
    "        # self.fusion_embedding_net.eval()\n",
    "        # checkpoint = torch.load(\"../RN-50-REFCOCOG.pt\")\n",
    "        \n",
    "        # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "        # checkpoint['model_state_dict'][\"input_resolution\"] = self.model.input_resolution #default is 224\n",
    "        # checkpoint['model_state_dict'][\"context_length\"] = self.model.context_length # default is 77\n",
    "        # checkpoint['model_state_dict'][\"vocab_size\"] = self.model.vocab_size \n",
    "\n",
    "        # self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.index_list_train = [i for i in range(0,len(self.annotation_train))]\n",
    "        self.index_list_val = [i for i in range(0,len(self.annotation_val))]\n",
    "        random.shuffle(self.index_list_train)\n",
    "        random.shuffle(self.index_list_val)\n",
    "\n",
    "\n",
    "    def _check_dataset(self):\n",
    "        if not os.path.exists(os.path.join(self.data_dir, self.ARCHIVE_NAME)):\n",
    "            if not os.path.exists(self.data_dir):\n",
    "                os.mkdir(self.data_dir)\n",
    "            print(\"Downloading dataset...\")\n",
    "            gdown.download(id=self.FILE_ID)\n",
    "        if not os.path.exists(os.path.join(self.data_dir, self.NAME)):\n",
    "            print(\"Extracting dataset...\")\n",
    "            with tarfile.open(\n",
    "                os.path.join(self.data_dir, self.ARCHIVE_NAME), \"r:gz\"\n",
    "            ) as tar:\n",
    "                tar.extractall(path=self.data_dir)\n",
    "        else:\n",
    "            print(\"Dataset already extracted\")\n",
    "\n",
    "\n",
    "    def _load_json(self):\n",
    "        with open(os.path.join(self.data_dir, self.NAME, self.JSON)) as f:\n",
    "            self.json = json.load(f)\n",
    "        self.json = pd.DataFrame(self.json[\"annotations\"])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idx,split = item\n",
    "        if split is None:\n",
    "            split = \"train\"\n",
    "        # get the random index from shuffled list\n",
    "        # get line by index\n",
    "        if split == \"train\":\n",
    "            random_index = self.index_list_train[idx]\n",
    "            raw = self.annotation_train.iloc[random_index]\n",
    "        elif split == \"val\":\n",
    "            random_index = self.index_list_val[idx]\n",
    "            raw = self.annotation_val.iloc[random_index]\n",
    "        else:\n",
    "            raise ValueError(\"split must be train or val!\")\n",
    "        # get image\n",
    "        image = self._get_image(raw)\n",
    "        # get sentences\n",
    "        \n",
    "        sentences = self._get_sentences(raw)\n",
    "        # get bbox\n",
    "\n",
    "        bboxes = self._get_bboxes(raw)\n",
    "\n",
    "        # return self._get_vector bboxes and image width and height\n",
    "        return self._get_vector(image, sentences) , bboxes, image.width, image.height, image, sentences\n",
    "\n",
    "\n",
    "    def _get_image(self, raw):\n",
    "        # get image_id\n",
    "        image_id = raw[\"image_id\"]\n",
    "        # pad image_id to 12 digits\n",
    "        image_id = str(image_id).zfill(12)\n",
    "        # convert image to tensor\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.data_dir, self.NAME, self.IMAGES, self.IMAGE_NAME.format(image_id)\n",
    "            )\n",
    "        )\n",
    "        return image\n",
    "\n",
    "\n",
    "    def _get_sentences(self, raw):\n",
    "        # get sentences\n",
    "        sentences = raw[\"sentences\"]\n",
    "        # get raw sentences\n",
    "        sentences = [sentence[\"raw\"] for sentence in sentences]\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def _get_bboxes(self, raw):\n",
    "        # get ref_id\n",
    "        id = raw[\"ann_id\"]\n",
    "        bboxes = self.json[self.json[\"id\"] == id][\"bbox\"].values[0]\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "    def _filter_annotation(self, path):\n",
    "        self.annotation = pd.read_pickle(path)\n",
    "        self.annotation = pd.DataFrame(self.annotation)\n",
    "        self.annotation_train = self.annotation[self.annotation[\"split\"] == \"train\"]\n",
    "        self.annotation_val = self.annotation[self.annotation[\"split\"] == \"val\"]\n",
    "\n",
    "\n",
    "    def _get_vector(self, image, sentences):\n",
    "            image = self.preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "            for i in range(0,len(sentences)):\n",
    "                text = \"a photo of \"+sentences[i]\n",
    "            text = clip.tokenize(sentences).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.encode_image(image)\n",
    "                text_features = self.model.encode_text(text)\n",
    "            text_features = torch.mean(text_features,dim=0).to(DEVICE)\n",
    "            # text_features = text_features.to(DEVICE).squeeze(0)\n",
    "            # image_features = image_features.to(DEVICE).squeeze(0)\n",
    "            # out = torch.cat((image_features, text_features),dim=0).to(DEVICE)\n",
    "            \n",
    "            # Combine image and text features and normalize\n",
    "            product = torch.mul(image_features, text_features)\n",
    "            power = torch.sign(product)* torch.sqrt(torch.abs(product))\n",
    "            out = torch.div(power, torch.norm(power, dim=1).reshape(-1, 1))\n",
    "            out =torch.mean(out,dim=0)\n",
    "\n",
    "            # print(out.shape, power_out.shape)\n",
    "            # append bbox\n",
    "            # print(f\"Output shape: {power_out.shape}\")\n",
    "            \n",
    "            # out = torch.cat((image_features, text_selected),dim=1).to(DEVICE).unsqueeze(0)\n",
    "            # return self.fusion_embedding_net(out).squeeze(0).squeeze(0).to(DEVICE)    \n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RefCOCOg(\".\", \"train\")\n",
    "val_dataset = RefCOCOg(\".\", \"val\")\n",
    "test_dataset = RefCOCOg(\".\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for the dataloader. Our batch size is set to *one*, because an agent processes one image at a time. The only way to process mutliple images at once would be to increase the number of agent, which would add so much overhead to the implementation just for reducing the training time, without improvement on the agent behavior, since the policy is updated after **5k** steps, and introducing a problem of managing the history of the parallel agents' actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ENVIRONMENT CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enum class used for actions rappresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@total_ordering\n",
    "class Actions(Enum):\n",
    "    ACT_RT = 0 #Right\n",
    "    ACT_LT = 1 #Left\n",
    "    ACT_UP = 2 #Up\n",
    "    ACT_DN = 3 #Down\n",
    "    ACT_TA = 4 #Taller\n",
    "    ACT_FA = 5 #Fatter\n",
    "    ACT_SR = 6 #Shorter\n",
    "    ACT_TH = 7 #Thiner\n",
    "    ACT_TR = 8 #Trigger \n",
    "\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.__class__ is other.__class__:\n",
    "            return self.value < other.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 OpenAI Gym\n",
    "\n",
    "When dealing with a reinforcement learning problem, we first need to define the environment where the agent will learn. We decided to use [OpenAI Gym](https://gym.openai.com/), a toolkit that provides convinient APIs to simplify our trainging process.\n",
    "\n",
    "First, we define our **observation** and **action** spaces: \n",
    "\n",
    "- **Observation space**: as described above, the agent state contains information about image, bounding box, bounding box image and history, with float values in a $R^1$ space.\n",
    "\n",
    "- **Action space**: as already mentioned, the agent has to choose one action among 9 possible actions, making it a discrete action space. \n",
    "\n",
    "And then, we have to define a **reset** and **step** function:\n",
    "\n",
    "- **Reset**: this function is called at the beginning of each episode, and it returns the initial state of the agent. In our case, based on the setup choosen, the initial state is representend by:\n",
    "\n",
    "    - the YOLO bounding box with the highest **cosine similarity** with the provided sentences.\n",
    "\n",
    "    - the full image bounding box\n",
    "\n",
    "- **Step**: this function is called at each step of the episode, and it returns the next state, the reward and a boolean value that indicates if the episode is over or not. In our case, after choosing the action, the agent will move the bounding box accordingly, compute the new **intersection over union** and use it to compute the new reward. The episode ends when the action choosed is ***TRIGGER*** or the agent has moved the bounding box \n",
    "\n",
    "    - **12 times** when starting from YOLO best bounding box\n",
    "    \n",
    "    - **50 times** when starting from the full image bounding box\n",
    "\n",
    "Eventually, we can define the **render** function, that will be used to visualize the agent's behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: esaustiva spiegazione del funzionamento dell'environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VisualGroundingEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    CONVERGENCE_THRESHOLD = 0.6\n",
    "\n",
    "\n",
    "    def __init__(self,dataset, num_agent=1,agent_offset=0,render_mode=None,random_validation=True):\n",
    "        # self.window_size = 512  # The size of the PyGame window\n",
    "        self.dataset = dataset\n",
    "        self.idx = {\"train\": 0, \"val\": 0}\n",
    "        self.iou = 0\n",
    "        self.current_iou=0\n",
    "        self.highest_iou=0\n",
    "        self.avg_similarity=0\n",
    "        self.num_agent=num_agent\n",
    "        self.agent_offset=agent_offset\n",
    "        self.idx[\"train\"]= self.agent_offset\n",
    "        self.idx[\"val\"]= self.agent_offset\n",
    "        self.max_steps_per_episode = 12\n",
    "        # self._max_episode_steps=50\n",
    "        self.steps_num=0\n",
    "        self.trigger_pressed=False\n",
    "        # wheter to extract images in random order or not \n",
    "        self.random_validation=random_validation\n",
    "        self.YOLO = YOLO(\"yolov8n.pt\")\n",
    "        self.observation_space = spaces.Box(low=-100000, high=100000, shape=(INPUT_SIZE,))\n",
    "        self.action_history = deque(maxlen = HISTORY_LENGTH)\n",
    "        # We have 9 actions, corresponding to \"right\", \"up\", \"left\", \"down\", \"v-shrink\", \"v-stretch\", \"h-shrink\", \"h-stretch\", \"confirm\"\n",
    "        self.action_space = spaces.Discrete(ACT_NUM)\n",
    "        print(\"Env initialized\")\n",
    "\n",
    "\n",
    "    def _get_info(self, trigger_pressed):\n",
    "        return {\"pred_bbox\": self._agent_location, \"target_bbox\": self._target_location, \"trigger_pressed\":trigger_pressed, \"img_idx\":self.idx[self.split]}\n",
    "\n",
    "\n",
    "    def compute_vbbox(self):\n",
    "      area_bbox = self.bbox_width * self.bbox_height\n",
    "      area_img = self.width * self.height\n",
    "      v_bbox = torch.tensor( [self.x1/(self.width-1), self.y1/(self.height-1), self.x2/(self.width-1), self.y2/(self.height-1), area_bbox/area_img])\n",
    "      return v_bbox\n",
    "\n",
    "\n",
    "    def history2vec(self):\n",
    "      \"\"\"\n",
    "      Convert action history deque to vector for constructing state\n",
    "      :param q: deque contains histories\n",
    "      \"\"\"\n",
    "      history = np.array(self.action_history)\n",
    "      res = np.zeros((HISTORY_LENGTH, ACT_NUM))\n",
    "      if len(history) != 0:\n",
    "        res[np.arange(len(history)) + (HISTORY_LENGTH - len(history)), history] = 1\n",
    "      res = res.reshape((HISTORY_LENGTH * ACT_NUM)).astype(np.float32)\n",
    "      return res\n",
    "    \n",
    "    \n",
    "    def select_best_segment_CLIP(self,image,sentences,n):\n",
    "      width, height = image.size[0], image.size[1]\n",
    "      sentences = clip.tokenize(sentences).to(DEVICE)\n",
    "      similarities = []\n",
    "      img_embeddings = []\n",
    "      coordinates = []\n",
    "      imgs=[]\n",
    "      with torch.no_grad():\n",
    "\n",
    "        # Wrapper function used to create the embedding\n",
    "        def create_embedding(image, coordinates):\n",
    "          image_crop = image.crop(coordinates)\n",
    "          imgs.append(image_crop)\n",
    "          image_emb = self.dataset.preprocess(image_crop).unsqueeze(0).to(DEVICE)\n",
    "          image_emb = self.dataset.model.encode_image(image_emb)\n",
    "          return image_emb\n",
    "\n",
    "        sentences_emb = self.dataset.model.encode_text(sentences).to(DEVICE)\n",
    "        # print(sentences_emb.shape)\n",
    "        sentences_emb = torch.mean(sentences_emb, dim=0,dtype=float).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        for i in range(n):\n",
    "          for j in range(n):\n",
    "            x1,y1,x2,y2 = j*width//n, i*height//n, (j+1)*width//n , (i+1)*height//n\n",
    "            coordinates.append((x1,y1,x2,y2))\n",
    "            img_embeddings.append(create_embedding(image, (x1,y1,x2,y2)))\n",
    "            # compute similarity between image and sentence\n",
    "            similarities.append(torch.cosine_similarity(img_embeddings[-1], sentences_emb,dim=-1).item())\n",
    "            if i+1 < n:\n",
    "                y2_new = (i+2)*height//n\n",
    "                coordinates.append((x1,y1,x2,y2_new))\n",
    "                img_embeddings.append(create_embedding(image,(x1,y1,x2,y2_new)))\n",
    "                similarities.append(torch.cosine_similarity(img_embeddings[-1], sentences_emb,dim=-1).item())\n",
    "            if j+1 < n:\n",
    "                x2_new = (j+2)*width//n\n",
    "                coordinates.append((x1,y1,x2_new,y2))\n",
    "                img_embeddings.append(create_embedding(image,(x1,y1,x2_new,y2)))\n",
    "                similarities.append(torch.cosine_similarity(img_embeddings[-1], sentences_emb,dim=-1).item())\n",
    "            if i+1 < n and j+1 < n:\n",
    "                y2_new = (i+2)*height//n\n",
    "                x2_new = (j+2)*width//n\n",
    "                coordinates.append((x1,y1,x2_new,y2_new))\n",
    "                img_embeddings.append(create_embedding(image,(x1,y1,x2_new,y2_new)))\n",
    "                similarities.append(torch.cosine_similarity(img_embeddings[-1], sentences_emb,dim=-1).item())\n",
    "        #retrieve the segment with the highest similarity\n",
    "        return img_embeddings[np.argmax(similarities)].squeeze(), coordinates[np.argmax(similarities)]\n",
    "\n",
    "\n",
    "    def select_best_segment_YOLO(self,img,sentences):\n",
    "\n",
    "      # Wrapper function used to create the embedding\n",
    "      def create_embedding(image, coordinates):\n",
    "          image_crop = image.crop(coordinates)\n",
    "          image_emb = self.dataset.preprocess(image_crop).unsqueeze(0).to(DEVICE)\n",
    "          image_emb = self.dataset.model.encode_image(image_emb)\n",
    "          return image_emb,image_crop\n",
    "\n",
    "      full_img_emb = self.dataset.preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "      # print(\"\\n\\n\\nSENTENCES: \",sentences)\n",
    "      text_emb = clip.tokenize(sentences).to(DEVICE)\n",
    "      with torch.no_grad():\n",
    "          full_img_emb = self.dataset.model.encode_image(full_img_emb)\n",
    "          text_emb = self.dataset.model.encode_text(text_emb)\n",
    "      # text_emb = text_emb[0]\n",
    "\n",
    "      text_emb = torch.mean(text_emb,dim=0,dtype=torch.float).to(DEVICE)\n",
    "      similarities=[]\n",
    "      img_embs=[]\n",
    "      coordinates=[]\n",
    "      # imgs=[]\n",
    "      #YOLO PREDICTIONS\n",
    "      cv2_img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "      # cv2.imshow(\"full image\",cv2_img)\n",
    "      results = self.YOLO.predict(cv2_img,verbose=False)\n",
    "      for r in results:\n",
    "          # annotator = Annotator(cv2_img)\n",
    "          boxes = r.boxes\n",
    "          for box in boxes:\n",
    "            # get box coordinates in (top, left, bottom, right) format\n",
    "              x1,y1,x2,y2 = box.xyxy[0]\n",
    "              pred_emb,_ = create_embedding(img,(int(x1),int(y1),int(x2),int(y2)))\n",
    "              coordinates.append((x1.item(),y1.item(),x2.item(),y2.item()))\n",
    "              img_embs.append(pred_emb)\n",
    "              # imgs.append(cv2_img[int(y1):int(y2) , int(x1):int(x2)])\n",
    "              # label = box.cls\n",
    "              # annotator.box_label((x1,y1,x2,y2), YOLO_.names[int(label)]+str(len(crops)-1))\n",
    "              similarities.append(torch.cosine_similarity(pred_emb, text_emb,dim=-1).item())\n",
    "              # img_result = annotator.result()\n",
    "              # NO PREDICTIONS -> RETURN FULL IMAGE\n",
    "      # cv2.imshow(\"YOLO\",imgs[np.argmax(similarities)])\n",
    "      # cv2.waitKey(1)\n",
    "      # tm.sleep(4)\n",
    "      if len(similarities) == 0:\n",
    "        return full_img_emb.squeeze(), (0,0,img.width,img.height)\n",
    "      else:\n",
    "        return img_embs[np.argmax(similarities)].squeeze(), coordinates[np.argmax(similarities)]\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # print(\"GIOU : \",self.current_iou,\"\\t\\t num steps: \",self.steps_num)\n",
    "        # if self.trigger_pressed:\n",
    "        #     print(\"TRIGGER\")\n",
    "        self.split = \"train\"\n",
    "        if options is not None:\n",
    "           self.split = options[\"split\"]\n",
    "        # print(\"GIOU : \",str(round(100*self.current_iou,3)),\"% with \",self.steps_num) #| AVG_SIMILARITY: \",str(round(100*self.avg_similarity,3))+\"%\")\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        \n",
    "        embeddings, bbox, width, height, image, sentences = self.dataset[self.idx[self.split],self.split]\n",
    "        self.sentences = sentences\n",
    "        self.image=image\n",
    "        self.img_txt_emb = embeddings\n",
    "        # dividing image into NUM_SEGMENTS segments (j*k lines) and choosing the one with the highest similarity\n",
    "        self.bbox_emb, (self.x1,self.y1,self.x2,self.y2) = self.select_best_segment_YOLO(self.image, self.sentences)#,int(math.sqrt(NUM_SEGMENTS)))\n",
    "        # print(\"Image has shape \",self.image.size,\" and bbox_emb agent is in \", (self.x1,self.y1,self.x2,self.y2))\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # Choose the agent's location uniformly\n",
    "        # torch.set_printoptions(threshold=10_000)\n",
    "        self.bbox_width = self.x2 - self.x1\n",
    "        self.bbox_height = self.y2 - self.y1\n",
    "        self._agent_location = torch.tensor([[self.x1, self.y1, self.x2, self.y2]]).to(DEVICE)\n",
    "\n",
    "        v_bbox = self.compute_vbbox().to(DEVICE)\n",
    "        bbox_x2 = bbox[0]+bbox[2]\n",
    "        bbox_y2 = bbox[1]+bbox[3]\n",
    "        self._target_location = torch.tensor([[bbox[0],bbox[1],bbox_x2,bbox_y2 ]]).to(DEVICE)\n",
    "        self.current_iou = torchvision.ops.distance_box_iou( self._agent_location ,self._target_location )[0].item()\n",
    "        self.iou = self.current_iou\n",
    "        self.current_max_iou = self.iou\n",
    "        self.highest_iou = self.current_iou\n",
    "        self.action_history = deque(maxlen=HISTORY_LENGTH)\n",
    "        \n",
    "        v_hist = torch.zeros(ACT_NUM*HISTORY_LENGTH).to(DEVICE)\n",
    "        state = torch.cat( (self.img_txt_emb.to(DEVICE),self.bbox_emb,v_bbox,v_hist) ).to(DEVICE)\n",
    "        self.steps_num = 0\n",
    "        # self.idx = self.idx + 1 \n",
    "        info = self._get_info(False)\n",
    "        self.idx[self.split] +=self.num_agent\n",
    "        \n",
    "        # reset dataset counter\n",
    "        if self.split==\"train\" and (self.idx[self.split] >= len(self.dataset.annotation_train)-1):\n",
    "          random.shuffle(self.dataset.index_list_train)\n",
    "          self.idx[self.split] = self.agent_offset\n",
    "\n",
    "        if self.split==\"val\" and self.random_validation:\n",
    "           self.idx[self.split] = random.randint(self.agent_offset,len(self.dataset.annotation_val)-1)\n",
    "        state =state.detach().cpu().numpy()\n",
    "        return state.squeeze(), info\n",
    "\n",
    "\n",
    "    def _update_bbox(self, action):\n",
    "      ALPHA = 0.2\n",
    "      BETA  = 0.1\n",
    "      self.x2 = self.x1 + self.bbox_width\n",
    "      self.y2 = self.y1 + self.bbox_height\n",
    "\n",
    "      assert action >= Actions.ACT_RT.value and action <= Actions.ACT_TR.value\n",
    "      if action <= Actions.ACT_DN.value:\n",
    "        delta_w = int(ALPHA * self.bbox_width)\n",
    "        delta_h = int(ALPHA * self.bbox_height)\n",
    "      else:\n",
    "        delta_w = int(BETA * self.bbox_width)\n",
    "        delta_h = int(BETA * self.bbox_height)\n",
    "      # PREVENT_STUCK:\n",
    "      if (delta_h == 0):\n",
    "        delta_h = 2\n",
    "      if (delta_w == 0):\n",
    "        delta_w = 2\n",
    "\n",
    "      if action == Actions.ACT_RT.value:\n",
    "        self.x1 += delta_w\n",
    "        self.x2 += delta_w\n",
    "      elif action == Actions.ACT_LT.value:\n",
    "        self.x1 -= delta_w\n",
    "        self.x2 -= delta_w\n",
    "      elif action == Actions.ACT_UP.value:\n",
    "        self.y1 -= delta_h\n",
    "        self.y2 -= delta_h\n",
    "      elif action == Actions.ACT_DN.value:\n",
    "        self.y1 += delta_h\n",
    "        self.y2 += delta_h\n",
    "      elif action == Actions.ACT_TA.value:\n",
    "        self.y1 -= delta_h\n",
    "        self.y2 += delta_h\n",
    "      elif action == Actions.ACT_FA.value:\n",
    "        self.x1 -= delta_w\n",
    "        self.x2 += delta_w\n",
    "      elif action == Actions.ACT_SR.value:\n",
    "        self.y1 += delta_h\n",
    "        self.y2 -= delta_h\n",
    "      elif action == Actions.ACT_TH.value:\n",
    "        self.x1 += delta_w\n",
    "        self.x2 -= delta_w\n",
    "      elif action == Actions.ACT_TR.value:\n",
    "        pass\n",
    "      else:\n",
    "        raise ValueError('Invalid action')\n",
    "      # ensure bbox inside image\n",
    "      if self.x1 < 0:\n",
    "        self.x1 = 0\n",
    "      if self.y1 < 0:\n",
    "        self.y1 = 0\n",
    "      if self.x2 < 0:\n",
    "        self.x2 = 0\n",
    "      if self.y2 < 0:\n",
    "        self.y2 = 0\n",
    "      if self.x1 >= self.width:\n",
    "        self.x1 = self.width - 1\n",
    "      if self.y1 >= self.height:\n",
    "        self.y1 = self.height - 1\n",
    "      if self.x2 >= self.width:\n",
    "        self.x2 = self.width - 1\n",
    "      if self.y2 >= self.height:\n",
    "        self.y2 = self.height - 1\n",
    "      # assert self.x1 < self.x2\n",
    "      # assert self.y1 < self.y2\n",
    "\n",
    "      # Ensure bbox doesn't get flipped\n",
    "      x1 = min(self.x1, self.x2)\n",
    "      y1 = min(self.y1, self.y2)\n",
    "      x2 = max(self.x1, self.x2)\n",
    "      y2 = max(self.y1, self.y2)\n",
    "      if x1 ==x2:\n",
    "          x2+=1\n",
    "      if y1 ==y2:\n",
    "          y2+=1\n",
    "\n",
    "      self.x1, self.y1, self.x2, self.y2 = x1, y1, x2, y2\n",
    "\n",
    "      return\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_num+=1\n",
    "        done = False\n",
    "        reward = 0\n",
    "        self.iou = self.current_iou\n",
    "\n",
    "        self._update_bbox(action)\n",
    "        # An episode is done iff the agent has reached the target\n",
    "\n",
    "        self.action_history.append(action)\n",
    "        v_hist = torch.from_numpy(self.history2vec()).to(DEVICE)\n",
    "        # print(\"action: \",action,\" -> current action hist: \",self.action_history)\n",
    "        # print(\"Step: current vhist: \",v_hist)\n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self._render_frame()\n",
    "        self.bbox_width = self.x2 - self.x1\n",
    "        self.bbox_height = self.y2 - self.y1\n",
    "        self._agent_location =  torch.tensor([[self.x1, self.y1, self.x2, self.y2]]).to(DEVICE)\n",
    "        v_bbox = self.compute_vbbox().to(DEVICE)\n",
    "        \n",
    "        agent_bbox_crop = self.image.crop( (self.x1, self.y1, self.x2, self.y2) )\n",
    "        img = self.dataset.preprocess(agent_bbox_crop).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            self.bbox_emb = self.dataset.model.encode_image(img).squeeze(0).to(DEVICE)\n",
    "        #state = torch.cat( (self.img_txt_emb.to(DEVICE),self.bbox_emb,v_bbox) ).to(DEVICE)\n",
    "        \n",
    "        state = torch.cat( (self.img_txt_emb.to(DEVICE),self.bbox_emb,v_bbox,v_hist) ).to(DEVICE)\n",
    "\n",
    "        self.current_iou = torchvision.ops.distance_box_iou( self._agent_location, self._target_location )[0].item()\n",
    "\n",
    "        # self.avg_similarity = torch.mean(bbox_similarity).item()\n",
    "        self.trigger_pressed = False\n",
    "        # if self.current_iou > VisualGroundingEnv.CONVERGENCE_THRESHOLD or self.steps_num > 49 or action==Actions.ACT_TR.value:\n",
    "        if self.steps_num >= self.max_steps_per_episode or action==Actions.ACT_TR.value:\n",
    "            done = True\n",
    "            if action==Actions.ACT_TR.value:\n",
    "                self.trigger_pressed = True\n",
    "            # if (self.current_iou > VisualGroundingEnv.CONVERGENCE_THRESHOLD):\n",
    "            #   print(f\"IOU% > {VisualGroundingEnv.CONVERGENCE_THRESHOLD*100}% ---> tot = {self.done_counter}\")\n",
    "\n",
    "        reward += self._calculate_reward(self.iou,self.current_iou,action==Actions.ACT_TR.value)\n",
    "        info = self._get_info(self.trigger_pressed)\n",
    "        state = state.detach().cpu().numpy()\n",
    "        return state, reward, done, info\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, previous_iou, current_iou, is_stop_action):\n",
    "      if is_stop_action:\n",
    "        if current_iou > VisualGroundingEnv.CONVERGENCE_THRESHOLD:\n",
    "          return 0.50  # Reward for correctly stopping when IoU has improved\n",
    "        else:\n",
    "          return -0.50  # Penalty for stopping when IoU has not improved\n",
    "      else:\n",
    "        iou_difference = current_iou - previous_iou\n",
    "        if iou_difference > 0:\n",
    "          return iou_difference  # Reward for an action that increases IoU\n",
    "        else:\n",
    "          return iou_difference  # Penalty for an action that decreases IoU\n",
    "\n",
    "        \n",
    "    def _calculate_reward(self, previous_iou, current_iou, is_stop_action):\n",
    "      #Get reward\n",
    "      if not is_stop_action:\n",
    "        if (current_iou > self.current_max_iou):\n",
    "          reward = current_iou\n",
    "          self.current_max_iou = current_iou\n",
    "        else:\n",
    "          reward = -0.1\n",
    "      else:\n",
    "        iou = torchvision.ops.box_iou( self._agent_location, self._target_location )[0].item()\n",
    "        if iou < VisualGroundingEnv.CONVERGENCE_THRESHOLD:\n",
    "          reward = -0.75\n",
    "        else:\n",
    "          reward = 0.75\n",
    "      reward += (-previous_iou + 0.98 * current_iou)\n",
    "      return reward\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. UTILITY FUNCTIONS DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions from 'normalization.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanStd:\n",
    "    # Dynamically calculate mean and std\n",
    "    def __init__(self, shape):  # shape:the dimension of input data\n",
    "        self.n = 0\n",
    "        self.mean = np.zeros(shape)\n",
    "        self.S = np.zeros(shape)\n",
    "        self.std = np.sqrt(self.S)\n",
    "\n",
    "\n",
    "    def update(self, x):\n",
    "        x = np.array(x)\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = x\n",
    "            self.std = x\n",
    "        else:\n",
    "            old_mean = self.mean.copy()\n",
    "            self.mean = old_mean + (x - old_mean) / self.n\n",
    "            self.S = self.S + (x - old_mean) * (x - self.mean)\n",
    "            self.std = np.sqrt(self.S / self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization:\n",
    "    def __init__(self, shape):\n",
    "        self.running_ms = RunningMeanStd(shape=shape)\n",
    "\n",
    "\n",
    "    def __call__(self, x, update=True):\n",
    "        # Whether to update the mean and std,during the evaluating,update=False\n",
    "        if update:\n",
    "            self.running_ms.update(x)\n",
    "        x = (x - self.running_ms.mean) / (self.running_ms.std + 1e-8)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RewardScaling:\n",
    "    def __init__(self, shape, gamma):\n",
    "        self.shape = shape  # reward shape=1\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.running_ms = RunningMeanStd(shape=self.shape)\n",
    "        self.R = np.zeros(self.shape)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.R = self.gamma * self.R + x\n",
    "        self.running_ms.update(self.R)\n",
    "        x = x / (self.running_ms.std + 1e-8)  # Only divided std\n",
    "        return x\n",
    "\n",
    "\n",
    "    def reset(self):  # When an episode is done,we should reset 'self.R'\n",
    "        self.R = np.zeros(self.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and classes from 'utils.py' (I removed dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionEmbedding(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(2048, 1024).to(DEVICE)\n",
    "    self.fc2 = nn.Linear(1024, 1024).to(DEVICE)\n",
    "    self.gelu = nn.GELU().to(DEVICE)\n",
    "    self.batch_norm = nn.BatchNorm1d(1024).to(DEVICE)\n",
    "\n",
    "\n",
    "  def forward(self, embedding):\n",
    "    # embedding.float32().to(DEVICE)\n",
    "    embedding = embedding.to(torch.float32).to(DEVICE)\n",
    "    fc1_out = self.fc1(embedding)\n",
    "    gelu_out = self.gelu(fc1_out)\n",
    "    gelu_out = gelu_out.permute(0,2,1)\n",
    "    norm_out = self.batch_norm(gelu_out)\n",
    "    norm_out = norm_out.permute(0,2,1)\n",
    "    fc2_out = self.fc2(norm_out)\n",
    "    out = self.gelu(fc2_out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img,x0_norm,y0_norm,x1_norm,y1_norm):\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    # print(\"den\",width,height,sep=\" | \")\n",
    "    x0 = int(x0_norm * width)\n",
    "    y0 = int(y0_norm * height)\n",
    "    x1 = int(x1_norm * width)\n",
    "    y1 = int(y1_norm * height)\n",
    "    return x0,y0,x1,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(img,x0,y0,x1,y1):\n",
    "    img = cv2.rectangle(img, (x0, y0), (x1, y1), (255,0,0), 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(agent, ground_truth):\n",
    "    iou =  torchvision.ops.box_iou( agent, ground_truth)[0].item()\n",
    "    print(\"iou : \",iou)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefCOCOg(Dataset):\n",
    "    FILE_ID = \"1wyyksgdLwnRMC9pQ-vjJnNUn47nWhyMD\"\n",
    "    ARCHIVE_NAME = \"refcocog.tar.gz\"\n",
    "    NAME = \"refcocog\"\n",
    "    ANNOTATIONS = \"annotations/refs(umd).p\"\n",
    "    JSON = \"annotations/instances.json\"\n",
    "    IMAGES = \"images\"\n",
    "    IMAGE_NAME = \"COCO_train2014_{}.jpg\"\n",
    "    FUSION_NET_PATH = \"fusion.pt\"\n",
    "\n",
    "\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self._check_dataset()\n",
    "        self._filter_annotation(\n",
    "            os.path.join(self.data_dir, self.NAME, self.ANNOTATIONS)\n",
    "        )\n",
    "        self._load_json()\n",
    "        self.transform = transform\n",
    "        self.model, self.preprocess = clip.load(\"RN50\", device=DEVICE)#,jit=False)\n",
    "        # ret = torch.load(\"fusion.pt\",map_location=DEVICE)\n",
    "        # self.fusion_embedding_net = FusionEmbedding()\n",
    "        # self.fusion_embedding_net.load_state_dict(torch.load(RefCOCOg.FUSION_NET_PATH)[\"model\"])\n",
    "        # self.fusion_embedding_net.eval()\n",
    "        # checkpoint = torch.load(\"../RN-50-REFCOCOG.pt\")\n",
    "        \n",
    "        # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "        # checkpoint['model_state_dict'][\"input_resolution\"] = self.model.input_resolution #default is 224\n",
    "        # checkpoint['model_state_dict'][\"context_length\"] = self.model.context_length # default is 77\n",
    "        # checkpoint['model_state_dict'][\"vocab_size\"] = self.model.vocab_size \n",
    "\n",
    "        # self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.index_list_train = [i for i in range(0,len(self.annotation_train))]\n",
    "        self.index_list_val = [i for i in range(0,len(self.annotation_val))]\n",
    "        random.shuffle(self.index_list_train)\n",
    "        random.shuffle(self.index_list_val)\n",
    "\n",
    "\n",
    "    def _check_dataset(self):\n",
    "        if not os.path.exists(os.path.join(self.data_dir, self.ARCHIVE_NAME)):\n",
    "            if not os.path.exists(self.data_dir):\n",
    "                os.mkdir(self.data_dir)\n",
    "            print(\"Downloading dataset...\")\n",
    "            gdown.download(id=self.FILE_ID)\n",
    "        if not os.path.exists(os.path.join(self.data_dir, self.NAME)):\n",
    "            print(\"Extracting dataset...\")\n",
    "            with tarfile.open(\n",
    "                os.path.join(self.data_dir, self.ARCHIVE_NAME), \"r:gz\"\n",
    "            ) as tar:\n",
    "                tar.extractall(path=self.data_dir)\n",
    "        else:\n",
    "            print(\"Dataset already extracted\")\n",
    "\n",
    "\n",
    "    def _load_json(self):\n",
    "        with open(os.path.join(self.data_dir, self.NAME, self.JSON)) as f:\n",
    "            self.json = json.load(f)\n",
    "        self.json = pd.DataFrame(self.json[\"annotations\"])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idx,split = item\n",
    "        if split is None:\n",
    "            split = \"train\"\n",
    "        # get the random index from shuffled list\n",
    "        # get line by index\n",
    "        if split == \"train\":\n",
    "            random_index = self.index_list_train[idx]\n",
    "            raw = self.annotation_train.iloc[random_index]\n",
    "        elif split == \"val\":\n",
    "            random_index = self.index_list_val[idx]\n",
    "            raw = self.annotation_val.iloc[random_index]\n",
    "        else:\n",
    "            raise ValueError(\"split must be train or val!\")\n",
    "        # get image\n",
    "        image = self._get_image(raw)\n",
    "        # get sentences\n",
    "        \n",
    "        sentences = self._get_sentences(raw)\n",
    "        # get bbox\n",
    "\n",
    "        bboxes = self._get_bboxes(raw)\n",
    "\n",
    "        # return self._get_vector bboxes and image width and height\n",
    "        return self._get_vector(image, sentences) , bboxes, image.width, image.height, image, sentences\n",
    "\n",
    "\n",
    "    def _get_image(self, raw):\n",
    "        # get image_id\n",
    "        image_id = raw[\"image_id\"]\n",
    "        # pad image_id to 12 digits\n",
    "        image_id = str(image_id).zfill(12)\n",
    "        # convert image to tensor\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.data_dir, self.NAME, self.IMAGES, self.IMAGE_NAME.format(image_id)\n",
    "            )\n",
    "        )\n",
    "        return image\n",
    "\n",
    "\n",
    "    def _get_sentences(self, raw):\n",
    "        # get sentences\n",
    "        sentences = raw[\"sentences\"]\n",
    "        # get raw sentences\n",
    "        sentences = [sentence[\"raw\"] for sentence in sentences]\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def _get_bboxes(self, raw):\n",
    "        # get ref_id\n",
    "        id = raw[\"ann_id\"]\n",
    "        bboxes = self.json[self.json[\"id\"] == id][\"bbox\"].values[0]\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "    def _filter_annotation(self, path):\n",
    "        self.annotation = pd.read_pickle(path)\n",
    "        self.annotation = pd.DataFrame(self.annotation)\n",
    "        self.annotation_train = self.annotation[self.annotation[\"split\"] == \"train\"]\n",
    "        self.annotation_val = self.annotation[self.annotation[\"split\"] == \"val\"]\n",
    "\n",
    "\n",
    "    def _get_vector(self, image, sentences):\n",
    "            image = self.preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "            for i in range(0,len(sentences)):\n",
    "                text = \"a photo of \"+sentences[i]\n",
    "            text = clip.tokenize(sentences).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.encode_image(image)\n",
    "                text_features = self.model.encode_text(text)\n",
    "            text_features = torch.mean(text_features,dim=0).to(DEVICE)\n",
    "            # text_features = text_features.to(DEVICE).squeeze(0)\n",
    "            # image_features = image_features.to(DEVICE).squeeze(0)\n",
    "            # out = torch.cat((image_features, text_features),dim=0).to(DEVICE)\n",
    "            \n",
    "            # Combine image and text features and normalize\n",
    "            product = torch.mul(image_features, text_features)\n",
    "            power = torch.sign(product)* torch.sqrt(torch.abs(product))\n",
    "            out = torch.div(power, torch.norm(power, dim=1).reshape(-1, 1))\n",
    "            out =torch.mean(out,dim=0)\n",
    "\n",
    "            # print(out.shape, power_out.shape)\n",
    "            # append bbox\n",
    "            # print(f\"Output shape: {power_out.shape}\")\n",
    "            \n",
    "            # out = torch.cat((image_features, text_selected),dim=1).to(DEVICE).unsqueeze(0)\n",
    "            # return self.fusion_embedding_net(out).squeeze(0).squeeze(0).to(DEVICE)    \n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayBuffer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, args):\n",
    "        self.gamma = args.gamma\n",
    "        self.lamda = args.lamda\n",
    "        self.use_adv_norm = args.use_adv_norm\n",
    "        self.state_dim = args.state_dim\n",
    "        self.action_dim = args.action_dim\n",
    "        self.episode_limit = args.episode_limit\n",
    "        self.batch_size = args.batch_size\n",
    "        self.episode_num = 0\n",
    "        self.max_episode_len = 0\n",
    "        self.buffer = None\n",
    "        self.reset_buffer()\n",
    "\n",
    "\n",
    "    def reset_buffer(self):\n",
    "        self.buffer = {'s': np.zeros([self.batch_size, self.episode_limit, self.state_dim]),\n",
    "                       'v': np.zeros([self.batch_size, self.episode_limit + 1]),\n",
    "                       'a': np.zeros([self.batch_size, self.episode_limit]),\n",
    "                       'a_logprob': np.zeros([self.batch_size, self.episode_limit]),\n",
    "                       'r': np.zeros([self.batch_size, self.episode_limit]),\n",
    "                       'dw': np.ones([self.batch_size, self.episode_limit]),  # Note: We use 'np.ones' to initialize 'dw'\n",
    "                       'active': np.zeros([self.batch_size, self.episode_limit])\n",
    "                       }\n",
    "        self.episode_num = 0\n",
    "        self.max_episode_len = 0\n",
    "\n",
    "\n",
    "    def store_transition(self, episode_step, s, v, a, a_logprob, r, dw):\n",
    "        self.buffer['s'][self.episode_num][episode_step] = s\n",
    "        self.buffer['v'][self.episode_num][episode_step] = v\n",
    "        self.buffer['a'][self.episode_num][episode_step] = a\n",
    "        self.buffer['a_logprob'][self.episode_num][episode_step] = a_logprob\n",
    "        self.buffer['r'][self.episode_num][episode_step] = r\n",
    "        self.buffer['dw'][self.episode_num][episode_step] = dw\n",
    "\n",
    "        self.buffer['active'][self.episode_num][episode_step] = 1.0\n",
    "\n",
    "\n",
    "    def store_last_value(self, episode_step, v):\n",
    "        self.buffer['v'][self.episode_num][episode_step] = v\n",
    "        self.episode_num += 1\n",
    "        # Record max_episode_len\n",
    "        if episode_step > self.max_episode_len:\n",
    "            self.max_episode_len = episode_step\n",
    "\n",
    "\n",
    "    def get_adv(self):\n",
    "        # Calculate the advantage using GAE\n",
    "        v = self.buffer['v'][:, :self.max_episode_len]\n",
    "        v_next = self.buffer['v'][:, 1:self.max_episode_len + 1]\n",
    "        r = self.buffer['r'][:, :self.max_episode_len]\n",
    "        dw = self.buffer['dw'][:, :self.max_episode_len]\n",
    "        active = self.buffer['active'][:, :self.max_episode_len]\n",
    "        adv = np.zeros_like(r)  # adv.shape=(batch_size,max_episode_len)\n",
    "        gae = 0\n",
    "        with torch.no_grad():  # adv and v_target have no gradient\n",
    "            # deltas.shape=(batch_size,max_episode_len)\n",
    "            deltas = r + self.gamma * v_next * (1 - dw) - v\n",
    "            for t in reversed(range(self.max_episode_len)):\n",
    "                gae = deltas[:, t] + self.gamma * self.lamda * gae  # gae.shape=(batch_size)\n",
    "                adv[:, t] = gae\n",
    "            v_target = adv + v  # v_target.shape(batch_size,max_episode_len)\n",
    "            if self.use_adv_norm:  # Trick 1:advantage normalization\n",
    "                adv_copy = copy.deepcopy(adv)\n",
    "                adv_copy[active == 0] = np.nan  # 忽略掉active=0的那些adv\n",
    "                adv = ((adv - np.nanmean(adv_copy)) / (np.nanstd(adv_copy) + 1e-5))\n",
    "        return adv, v_target\n",
    "\n",
    "\n",
    "    def get_training_data(self):\n",
    "        adv, v_target = self.get_adv()\n",
    "        batch = {'s': torch.tensor(self.buffer['s'][:, :self.max_episode_len], dtype=torch.float32),\n",
    "                 'a': torch.tensor(self.buffer['a'][:, :self.max_episode_len], dtype=torch.long),  # 动作a的类型必须是long\n",
    "                 'a_logprob': torch.tensor(self.buffer['a_logprob'][:, :self.max_episode_len], dtype=torch.float32),\n",
    "                 'active': torch.tensor(self.buffer['active'][:, :self.max_episode_len], dtype=torch.float32),\n",
    "                 'adv': torch.tensor(adv, dtype=torch.float32),\n",
    "                 'v_target': torch.tensor(v_target, dtype=torch.float32)}\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick 8: orthogonal initialization\n",
    "def orthogonal_init(layer, gain=np.sqrt(2)):\n",
    "    for name, param in layer.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, 0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.orthogonal_(param, gain=gain)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Critic_RNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Actor_Critic_RNN, self).__init__()\n",
    "        self.use_gru = args.use_gru\n",
    "        self.transformer = args.transformer\n",
    "        self.activate_func = [nn.ReLU(), nn.Tanh()][args.use_tanh]  # Trick10: use tanh\n",
    "        self.actor_rnn_hidden = None\n",
    "        self.actor_fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.actor_fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        if args.transformer:\n",
    "            self.actor_trans_key = nn.Linear(args.hidden_dim, args.hidden_dim).to(DEVICE)\n",
    "            self.actor_trans_value = nn.Linear(args.hidden_dim, args.hidden_dim).to(DEVICE)\n",
    "            self.actor_trans_query = nn.Linear(args.hidden_dim, args.hidden_dim).to(DEVICE)\n",
    "            self.actor_attention = nn.MultiheadAttention(args.hidden_dim, 8).to(DEVICE)\n",
    "        else:\n",
    "            if args.use_gru:\n",
    "                print(\"------use GRU------\")\n",
    "                self.actor_rnn = nn.GRU(args.hidden_dim, args.hidden_dim, batch_first=True)\n",
    "            else:\n",
    "                print(\"------use LSTM------\")\n",
    "                self.actor_rnn = nn.LSTM(args.hidden_dim, args.hidden_dim, batch_first=True)\n",
    "        self.actor_fc3 = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "\n",
    "        self.critic_rnn_hidden = None\n",
    "        self.critic_fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.critic_fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        \n",
    "        if args.transformer:\n",
    "            print(\"------use transformer------\")\n",
    "            self.critic_trans_key = nn.Linear(args.hidden_dim, args.hidden_dim).to(DEVICE)\n",
    "            self.critic_trans_value = nn.Linear(args.hidden_dim, args.hidden_dim).to(DEVICE)\n",
    "            self.critic_trans_query = nn.Linear(args.hidden_dim, args.hidden_dim).to(DEVICE)\n",
    "            self.critic_attention = nn.MultiheadAttention(args.hidden_dim, 8).to(DEVICE)\n",
    "        else:\n",
    "            if args.use_gru:\n",
    "                self.critic_rnn = nn.GRU(args.hidden_dim, args.hidden_dim, batch_first=True)\n",
    "            else:\n",
    "                self.critic_rnn = nn.LSTM(args.hidden_dim, args.hidden_dim,batch_first=True)\n",
    "        self.critic_fc3 = nn.Linear(args.hidden_dim, 1)\n",
    "\n",
    "        if args.use_orthogonal_init:\n",
    "            print(\"------use orthogonal init------\")\n",
    "            orthogonal_init(self.actor_fc1)\n",
    "            orthogonal_init(self.actor_fc2, gain=0.01)\n",
    "            orthogonal_init(self.actor_fc3)\n",
    "            orthogonal_init(self.critic_fc1)\n",
    "            orthogonal_init(self.critic_fc2)\n",
    "            orthogonal_init(self.critic_fc3)\n",
    "            if args.transformer:\n",
    "                orthogonal_init(self.actor_trans_key)\n",
    "                orthogonal_init(self.actor_trans_value)\n",
    "                orthogonal_init(self.actor_trans_query)\n",
    "                orthogonal_init(self.actor_attention)\n",
    "                orthogonal_init(self.critic_trans_key)\n",
    "                orthogonal_init(self.critic_trans_value)\n",
    "                orthogonal_init(self.critic_trans_query)\n",
    "                orthogonal_init(self.critic_attention)\n",
    "            else:\n",
    "                orthogonal_init(self.actor_rnn)\n",
    "                orthogonal_init(self.critic_rnn)\n",
    "\n",
    "\n",
    "    def actor(self, s):\n",
    "        s = s.to(DEVICE)\n",
    "        s = self.actor_fc1(s)\n",
    "        s = self.activate_func(s)\n",
    "        s = self.actor_fc2(s)\n",
    "        s = self.activate_func(s)\n",
    "        if self.transformer:\n",
    "            key = self.actor_trans_key(s)\n",
    "            value = self.actor_trans_value(s)\n",
    "            query = self.actor_trans_query(s)\n",
    "            output, attention_weight = self.actor_attention(query, key, value)\n",
    "        else:\n",
    "            output, self.actor_rnn_hidden = self.actor_rnn(s, self.actor_rnn_hidden)\n",
    "        logit = self.actor_fc3(output)\n",
    "        return logit\n",
    "\n",
    "\n",
    "    def critic(self, s):\n",
    "        s = s.to(DEVICE)\n",
    "        s = self.critic_fc1(s)\n",
    "        s = self.activate_func(s)\n",
    "        s = self.critic_fc2(s)\n",
    "        s = self.activate_func(s)\n",
    "        if self.transformer:\n",
    "            key = self.critic_trans_key(s)\n",
    "            value = self.critic_trans_value(s)\n",
    "            query = self.critic_trans_query(s)\n",
    "            output, attention_weight = self.critic_attention(query, key, value)\n",
    "        else:\n",
    "            output, self.critic_rnn_hidden = self.critic_rnn(s, self.critic_rnn_hidden)\n",
    "        value = self.critic_fc3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_discrete_RNN:\n",
    "    def __init__(self, args):\n",
    "        self.batch_size = args.batch_size\n",
    "        self.mini_batch_size = args.mini_batch_size\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        self.lr = args.lr  # Learning rate of actor\n",
    "        self.gamma = args.gamma  # Discount factor\n",
    "        self.lamda = args.lamda  # GAE parameter\n",
    "        self.epsilon = args.epsilon  # PPO clip parameter\n",
    "        self.K_epochs = args.K_epochs  # PPO parameter\n",
    "        self.entropy_coef = args.entropy_coef  # Entropy coefficient\n",
    "        self.set_adam_eps = args.set_adam_eps\n",
    "        self.use_grad_clip = args.use_grad_clip\n",
    "        self.use_lr_decay = args.use_lr_decay\n",
    "        self.use_adv_norm = args.use_adv_norm\n",
    "\n",
    "        self.ac = Actor_Critic_RNN(args).cuda()\n",
    "        if self.set_adam_eps:  # Trick 9: set Adam epsilon=1e-5\n",
    "            self.optimizer = torch.optim.Adam(self.ac.parameters(), lr=self.lr, eps=1e-5)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.Adam(self.ac.parameters(), lr=self.lr)\n",
    "\n",
    "    def reset_rnn_hidden(self):\n",
    "        self.ac.actor_rnn_hidden = None\n",
    "        self.ac.critic_rnn_hidden = None\n",
    "\n",
    "    def choose_action(self, s, evaluate=False):\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor(s, dtype=torch.float).unsqueeze(0)\n",
    "            logit = self.ac.actor(s)\n",
    "            if evaluate:\n",
    "                a = torch.argmax(logit)\n",
    "                return a.item(), None\n",
    "            else:\n",
    "                dist = Categorical(logits=logit)\n",
    "                a = dist.sample()\n",
    "                a_logprob = dist.log_prob(a)\n",
    "                return a.item(), a_logprob.item()\n",
    "\n",
    "    def get_value(self, s):\n",
    "        with torch.no_grad():\n",
    "            s = torch.tensor(s, dtype=torch.float).unsqueeze(0)\n",
    "            value = self.ac.critic(s)\n",
    "            return value.item()\n",
    "\n",
    "    def train(self, replay_buffer, total_steps):\n",
    "        batch = replay_buffer.get_training_data()  # Get training data\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            for index in BatchSampler(SequentialSampler(range(self.batch_size)), self.mini_batch_size, False):\n",
    "                # If use RNN, we need to reset the rnn_hidden of the actor and critic.\n",
    "                self.reset_rnn_hidden()\n",
    "                logits_now = self.ac.actor(batch['s'][index])  # logits_now.shape=(mini_batch_size, max_episode_len, action_dim)\n",
    "                values_now = self.ac.critic(batch['s'][index]).squeeze(-1)  # values_now.shape=(mini_batch_size, max_episode_len)\n",
    "\n",
    "                dist_now = Categorical(logits=logits_now)\n",
    "                dist_entropy = dist_now.entropy()  # shape(mini_batch_size, max_episode_len)\n",
    "                a_logprob_now = dist_now.log_prob(batch['a'][index])  # shape(mini_batch_size, max_episode_len)\n",
    "                # a/b=exp(log(a)-log(b))\n",
    "                ratios = torch.exp(a_logprob_now - batch['a_logprob'][index])  # shape(mini_batch_size, max_episode_len)\n",
    "\n",
    "                # actor loss\n",
    "                surr1 = ratios * batch['adv'][index]\n",
    "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * batch['adv'][index]\n",
    "                actor_loss = -torch.min(surr1, surr2) - self.entropy_coef * dist_entropy  # shape(mini_batch_size, max_episode_len)\n",
    "                actor_loss = (actor_loss * batch['active'][index]).sum() / batch['active'][index].sum()\n",
    "\n",
    "                # critic_loss\n",
    "                critic_loss = (values_now - batch['v_target'][index]) ** 2\n",
    "                critic_loss = (critic_loss * batch['active'][index]).sum() / batch['active'][index].sum()\n",
    "\n",
    "                # Update\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = actor_loss + critic_loss * 0.5\n",
    "                loss.backward()\n",
    "                if self.use_grad_clip:  # Trick 7: Gradient clip\n",
    "                    torch.nn.utils.clip_grad_norm_(self.ac.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "        if self.use_lr_decay:  # Trick 6:learning rate Decay\n",
    "            self.lr_decay(total_steps)\n",
    "    \n",
    "    def lr_decay(self, total_steps):\n",
    "        lr_now = 0.9 * self.lr * (1 - total_steps / self.max_train_steps) + 0.1 * self.lr\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr_now\n",
    "\n",
    "    def save_model(self, env_name, number, total_steps):\n",
    "        torch.save(self.ac.state_dict(), \"model/PPO_actor_env_{}_number_{}_step_{}k.pth\".format(env_name, number, total_steps))\n",
    "\n",
    "    def load_model(self, env_name, number,step_num=None):\n",
    "        # extract \n",
    "        if step_num >0:\n",
    "            step = step_num\n",
    "        else:\n",
    "            step = max([int(x.split(\"_\")[-1][:-5]) for x in os.listdir(\"model\")])\n",
    "        # step = \"final\"\n",
    "        print(\"LOADING model/PPO_actor_env_{}_number_{}_step_{}k.pth\".format(env_name, number, step))\n",
    "        self.ac.load_state_dict(torch.load(\"model/PPO_actor_env_{}_number_{}_step_{}k.pth\".format(env_name, number, step), map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self, args, env_name, number, seed):\n",
    "        self.args = args\n",
    "        self.env_name = env_name\n",
    "        self.number = number\n",
    "        self.seed = seed\n",
    "        self.dataset = None\n",
    "        self.save_model_freq = self.args.save_model_freq\n",
    "        self.save_counter = 0\n",
    "        self.last_save_counter=0\n",
    "        self.load_last_weights = self.args.resume\n",
    "        self.set_dataset()\n",
    "\n",
    "        # Create env\n",
    "        self.env = gym.make(env_name,dataset=self.dataset,num_agent=1)\n",
    "        # Set random seed\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        # self.env.seed(seed)\n",
    "        # self.env.action_space.seed(seed)\n",
    "\n",
    "        self.args.state_dim = self.env.observation_space.shape[0]\n",
    "        self.args.action_dim = self.env.action_space.n\n",
    "        self.args.episode_limit = self.env.max_steps_per_episode  # Maximum number of steps per episode\n",
    "        print(\"env={}\".format(env_name))\n",
    "        print(\"state_dim={}\".format(args.state_dim))\n",
    "        print(\"action_dim={}\".format(args.action_dim))\n",
    "        print(\"episode_limit={}\".format(args.episode_limit))\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(args)\n",
    "        self.agent = PPO_discrete_RNN(args)\n",
    "        # Create a tensorboard\n",
    "\n",
    "        self.evaluate_rewards = []  # Record the rewards during the evaluating\n",
    "        self.total_steps = 0\n",
    "        if self.load_last_weights:\n",
    "            print(\"------Resuming train from step #\",self.args.steps_num,\"------\")\n",
    "            self.agent.load_model(\"VisualGrounding-v0\", self.number,self.args.steps_num)\n",
    "            self.total_steps = self.args.steps_num\n",
    "\n",
    "        if self.args.use_state_norm:\n",
    "            print(\"------use state normalization------\")\n",
    "            self.state_norm = Normalization(shape=args.state_dim)  # Trick 2:state normalization\n",
    "        if self.args.use_reward_scaling:\n",
    "            print(\"------use reward scaling------\")\n",
    "            self.reward_scaling = RewardScaling(shape=1, gamma=self.args.gamma)\n",
    "        print(\"RUNNER INITIALIZED\")\n",
    "\n",
    "\n",
    "    def set_dataset(self):\n",
    "        self.dataset = RefCOCOg(data_dir=\"../\")\n",
    "\n",
    "\n",
    "    def run(self, ):\n",
    "        evaluate_num = -1  # Record the number of evaluations\n",
    "        if self.load_last_weights:\n",
    "            evaluate_num = self.total_steps // self.args.evaluate_freq\n",
    "        while self.total_steps < self.args.max_train_steps:\n",
    "            print(\"tot:{}, evaluate_num:{} , freq: {}\".format(self.total_steps, evaluate_num, self.args.evaluate_freq))\n",
    "            if self.total_steps // self.args.evaluate_freq > evaluate_num:\n",
    "                self.evaluate_policy()  # Evaluate the policy every 'evaluate_freq' steps\n",
    "                evaluate_num += 1\n",
    "            _, episode_steps = self.run_episode()  # Run an episode\n",
    "            self.total_steps += episode_steps\n",
    "\n",
    "            if self.replay_buffer.episode_num == self.args.batch_size:\n",
    "                self.agent.train(self.replay_buffer, self.total_steps)  # Training\n",
    "                self.replay_buffer.reset_buffer()\n",
    "\n",
    "        self.evaluate_policy()\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def run_episode(self, ):\n",
    "        episode_reward = 0\n",
    "        s,info = self.env.reset(options={\"split\":\"train\"})\n",
    "        if self.args.use_reward_scaling:\n",
    "            self.reward_scaling.reset()\n",
    "        self.agent.reset_rnn_hidden()\n",
    "        for episode_step in range(self.args.episode_limit):\n",
    "            if self.args.use_state_norm:\n",
    "                s = self.state_norm(s)\n",
    "            a, a_logprob = self.agent.choose_action(s, evaluate=False)\n",
    "            v = self.agent.get_value(s)\n",
    "            s_, r, done, info = self.env.step(a)\n",
    "            episode_reward += r\n",
    "\n",
    "            if done and episode_step + 1 != self.args.episode_limit:\n",
    "                dw = True\n",
    "            else:\n",
    "                dw = False\n",
    "            if self.args.use_reward_scaling:\n",
    "                r = self.reward_scaling(r)\n",
    "            # Store the transition\n",
    "            self.replay_buffer.store_transition(episode_step, s, v, a, a_logprob, r, dw)\n",
    "            s = s_\n",
    "            if done==True:\n",
    "                break\n",
    "\n",
    "        # An episode is over, store v in the last step\n",
    "        if self.args.use_state_norm:\n",
    "            s = self.state_norm(s)\n",
    "        v = self.agent.get_value(s)\n",
    "        self.replay_buffer.store_last_value(episode_step + 1, v)\n",
    "        \n",
    "        \n",
    "        return episode_reward, episode_step + 1\n",
    "\n",
    "    \n",
    "    def evaluate_policy(self, ):\n",
    "        print(\"EVALUATION START\")\n",
    "        evaluate_reward = 0\n",
    "        evaluate_iou = 0\n",
    "        iou_step = 0\n",
    "        for _ in range(self.args.evaluate_times):\n",
    "            episode_reward, done, info = 0, False, {}\n",
    "            s, info = self.env.reset(options={\"split\":\"val\"})\n",
    "            counter = 0\n",
    "            self.agent.reset_rnn_hidden()\n",
    "            while done==False and counter<self.args.episode_limit :\n",
    "                if self.args.use_state_norm:\n",
    "                    s = self.state_norm(s, update=False)\n",
    "                a, a_logprob = self.agent.choose_action(s, evaluate=True)\n",
    "                s_, r, done, info = self.env.step(a)\n",
    "                episode_reward += r\n",
    "                s = s_\n",
    "                # print(\"trigger is \",done, \" | #\", counter,\" steps\", counter<=self.args.episode_limit)\n",
    "                counter+=1\n",
    "            print(\"episode_reward: \",episode_reward, \"\\t|\\t num_actions: \",counter)\n",
    "            evaluate_reward += episode_reward\n",
    "            iou = torchvision.ops.box_iou(torch.tensor(info[\"target_bbox\"]).to(DEVICE),torch.tensor(info[\"pred_bbox\"]).to(DEVICE)).item()\n",
    "            evaluate_iou += iou\n",
    "            iou_step += 1\n",
    "        evaluate_reward = evaluate_reward / self.args.evaluate_times\n",
    "        self.evaluate_rewards.append(evaluate_reward)\n",
    "        print(\"total_steps:{} \\t mean_iou: {}evaluate_reward:{}\".format(self.total_steps,evaluate_iou/iou_step, evaluate_reward))\n",
    "        # Save the rewards and models\n",
    "        wandb.log( { \"mean_iou\":evaluate_iou/iou_step, \"mean_reward\": evaluate_reward},step=self.total_steps )\n",
    "        if self.total_steps - self.last_save_counter > self.save_model_freq:\n",
    "            print(\"SAVING MODEL\")\n",
    "            self.agent.save_model(self.env_name, self.number, self.total_steps)\n",
    "            self.last_save_counter = self.total_steps\n",
    "        print(\"Total steps: \",self.total_steps)\n",
    "        print(\"EVALUATION END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: sistemare parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda:0\")\n",
    "gym.envs.register(\n",
    "    id='VisualGrounding-v0',\n",
    "    entry_point='env.env:VisualGroundingEnv'\n",
    ")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Hyperparameter Setting for PPO-discrete\")\n",
    "parser.add_argument(\"-n\", type=int, default=int(-1), help=\" name index\")\n",
    "parser.add_argument(\"--max_train_steps\", type=int, default=int(2e6), help=\" Maximum number of training steps\")\n",
    "parser.add_argument(\"--evaluate_freq\", type=float, default=5e3, help=\"Evaluate the policy every 'evaluate_freq' steps\")\n",
    "parser.add_argument(\"--save_model_freq\", type=int, default=2e4, help=\"Save frequency\")\n",
    "parser.add_argument(\"--evaluate_times\", type=float, default=100, help=\"Evaluate times\")\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1024, help=\"Batch size\")\n",
    "parser.add_argument(\"--mini_batch_size\", type=int, default=128, help=\"Minibatch size\")\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=1024, help=\"The number of neurons in hidden layers of the neural network\")\n",
    "parser.add_argument(\"--lr\", type=float, default=3e-4, help=\"Learning rate of actor\")\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor\")\n",
    "parser.add_argument(\"--lamda\", type=float, default=0.95, help=\"GAE parameter\")\n",
    "parser.add_argument(\"--epsilon\", type=float, default=0.20, help=\"PPO clip parameter\")\n",
    "parser.add_argument(\"--K_epochs\", type=int, default=15, help=\"PPO parameter\")\n",
    "parser.add_argument(\"--use_adv_norm\", type=bool, default=True, help=\"Trick 1:advantage normalization\")\n",
    "parser.add_argument(\"--use_state_norm\", type=bool, default=False, help=\"Trick 2:state normalization\")\n",
    "parser.add_argument(\"--use_reward_scaling\", type=bool, default=True, help=\"Trick 4:reward scaling\")\n",
    "parser.add_argument(\"--entropy_coef\", type=float, default=0.01, help=\"Trick 5: policy entropy\")\n",
    "parser.add_argument(\"--use_lr_decay\", type=bool, default=True, help=\"Trick 6:learning rate Decay\")\n",
    "parser.add_argument(\"--use_grad_clip\", type=bool, default=True, help=\"Trick 7: Gradient clip\")\n",
    "parser.add_argument(\"--use_orthogonal_init\", type=bool, default=True, help=\"Trick 8: orthogonal initialization\")\n",
    "parser.add_argument(\"--set_adam_eps\", type=float, default=True, help=\"Trick 9: set Adam epsilon=1e-5\")\n",
    "parser.add_argument(\"--use_tanh\", type=float, default=True, help=\"Trick 10: tanh activation function\")\n",
    "parser.add_argument(\"--use_gru\", type=bool, default=False, help=\"Whether to use GRU\")\n",
    "parser.add_argument(\"--resume\", type=bool, default=False, help=\"load last weights and resume training from checkpoint\")\n",
    "parser.add_argument(\"--steps_num\", type=int, default=-1, help=\"steps number to load weights\")\n",
    "parser.add_argument(\"--transformer\", type=bool, default=False, help=\"whether to use transformer instead of lstm\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# Jupyter Notebook argparse compatibility test\n",
    "args = argparser.Namespace(\n",
    "    n = int(-1),\n",
    "    max_train_steps = int(2e6),\n",
    "    evaluate_freq = 5e3,\n",
    "    save_model_freq = 2e4,\n",
    "    evaluate_times = 100,\n",
    "    batch_size = 1024,\n",
    "    mini_batch_size = 128,\n",
    "    hidden_dim = 1024,\n",
    "    lr = 3e-4,\n",
    "    gamma = 0.99,\n",
    "    lamda = 0.95,\n",
    "    epsilon = 0.20,\n",
    "    K_epochs = 15,\n",
    "    use_adv_norm = True,\n",
    "    use_state_norm = False,\n",
    "    use_reward_scaling = True,\n",
    "    entropy_coef = 0.01,\n",
    "    use_lr_decay = True,\n",
    "    use_grad_clip = True,\n",
    "    use_orthogonal_init = True,\n",
    "    set_adam_eps = True,\n",
    "    use_tanh = True,\n",
    "    use_gru = False,\n",
    "    resume = False,\n",
    "    steps_num = -1,\n",
    "    transformer = False,\n",
    "    )\n",
    "print(args)\n",
    "\n",
    "if args.n == -1:\n",
    "    print(\"Please input the index of the model\")\n",
    "    exit()\n",
    "if args.steps_num == -1 and args.resume==True:\n",
    "    print(\"Please input the steps number of the checkpoint\")\n",
    "    exit()\n",
    "run = wandb.init(project=\"lstm-RL-PPO\",id=\"12eps+LSTM+Trigger-Fixed+YOLOProp\",resume=args.resume, config={\n",
    "    \"lstm_hidden_size\":1024,\n",
    "    \"batch_size\":1024,\n",
    "    \"lr\":3e-4,\n",
    "    \"gamma\":0.99,\n",
    "    \"mini_batch_size\":64,\n",
    "    \"episode_limit\":12,\n",
    "    \"use_history\":True,\n",
    "    \"epsilon\":0.2,\n",
    "    \"embedding_fusion\":False,\n",
    "    \"convergence_threshold\":0.5\n",
    "})\n",
    "env_names = ['VisualGrounding-v0']\n",
    "env_index = 0\n",
    "for seed in [0, 10, 100]:\n",
    "    runner = Runner(args, env_name=env_names[env_index], number=args.n, seed=seed)\n",
    "    runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img,x0_norm,y0_norm,x1_norm,y1_norm):\n",
    "    width = img.shape[1]\n",
    "    height = img.shape[0]\n",
    "    # print(\"den\",width,height,sep=\" | \")\n",
    "    x0 = int(x0_norm * width)\n",
    "    y0 = int(y0_norm * height)\n",
    "    x1 = int(x1_norm * width)\n",
    "    y1 = int(y1_norm * height)\n",
    "    return x0,y0,x1,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(agent, ground_truth):\n",
    "    iou =  torchvision.ops.box_iou( agent, ground_truth)[0].item()\n",
    "    print(\"iou : \",iou)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {\n",
    "    0: 'right',\n",
    "    1: 'left',\n",
    "    2: 'up',\n",
    "    3: 'down',\n",
    "    4: 'taller',\n",
    "    5: 'fatter',\n",
    "    6: 'shorter',\n",
    "    7: 'thinner',\n",
    "    8: 'trigger'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    def __init__(self, args, number):\n",
    "        self.args = args\n",
    "        gym.envs.register(\n",
    "            id='VisualGrounding-v0',\n",
    "            entry_point='env.env:VisualGroundingEnv'\n",
    "            )\n",
    "        self.number = number #id of model\n",
    "        self.dataset = RefCOCOg(\"../\",\"val\")\n",
    "        self.env = gym.make(\"VisualGrounding-v0\",dataset=self.dataset,num_agent=1,random_validation=False)\n",
    "        self.args.state_dim = self.env.observation_space.shape[0]\n",
    "        self.args.action_dim = self.env.action_space.n\n",
    "        self.args.episode_limit = self.env.max_steps_per_episode\n",
    "        self.agent = PPO_discrete_RNN(self.args)\n",
    "        self.agent.load_model(\"VisualGrounding-v0\", self.number,self.args.steps_num)\n",
    "        self.state_norm = Normalization(shape=args.state_dim)  # Trick 2:state normalization\n",
    "\n",
    "\n",
    "    def show_result(self,image,sentences,info):\n",
    "        gt_x0,gt_y0,gt_x1,gt_y1 = int(info[\"target_bbox\"][0][0].item()),int(info[\"target_bbox\"][0][1].item()),int(info[\"target_bbox\"][0][2].item()),int(info[\"target_bbox\"][0][3].item())\n",
    "        x0,y0,x1,y1 = int(info[\"pred_bbox\"][0][0].item()),int(info[\"pred_bbox\"][0][1].item()),int(info[\"pred_bbox\"][0][2].item()),int(info[\"pred_bbox\"][0][3].item())\n",
    "        #draw predicted bbox\n",
    "        bbox_img = cv2.rectangle(np.array(image), (int(x0), int(y0)), (int(x1), int(y1)), (255,0,0), 2)\n",
    "        #draw ground truth\n",
    "        bbox_img = cv2.rectangle(bbox_img,(int(gt_x0),int(gt_y0)),(int(gt_x1),int(gt_y1)), (0,0,255), 2)\n",
    "        \n",
    "        # bbox_img = cv2.resize(bbox_img, (int(bbox_img.shape[1]/3), int(bbox_img.shape[0]/3)))\n",
    "\n",
    "        Image.fromarray(bbox_img).show()\n",
    "        print(\"Groud Truth: {:>5} {:>5} {:>5} {:>5}  |    agent_location: {:>5} {:>5} {:>5} {:>5}\".format(gt_x0,gt_y0,gt_x1,gt_y1,x0,y0,x1,y1))\n",
    "        print(\"sentences:\\n\",sentences)\n",
    "        tm.sleep(7)\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        evaluate_reward = 0\n",
    "        evaluate_iou, iou_count = 0, 0\n",
    "        accuracy_count = 0\n",
    "        for i in range(0,VALIDATION_SET_SIZE):\n",
    "            episode_reward, done, info = 0, False, {}\n",
    "            s, info = self.env.reset(options={\"split\":\"val\"})\n",
    "            img_idx = info[\"img_idx\"]\n",
    "            _,_,_,_, image, sentences= self.dataset[img_idx,\"val\"]\n",
    "            self.agent.reset_rnn_hidden()\n",
    "            counter = 0\n",
    "            actions=[]\n",
    "            \n",
    "            while info[\"trigger_pressed\"]==False and  counter < 12:\n",
    "                if self.args.use_state_norm:\n",
    "                    s = self.state_norm(s)\n",
    "                a, a_logprob = self.agent.choose_action(s, evaluate=True)\n",
    "                s_, r, done, info = self.env.step(a)\n",
    "                print(\"received reward: \",r)\n",
    "                episode_reward += r\n",
    "                s = s_\n",
    "                counter+=1\n",
    "                # print(\"Episode reward: \",episode_reward)\n",
    "                actions.append(action_dict[a])\n",
    "                if a == 8:\n",
    "                    print(\"Trigger pressed after {} steps\".format(counter))\n",
    "                    # self.show_result(image,sentences,info)\n",
    "            print(\"Episode reward: \",episode_reward)\n",
    "            iou = torchvision.ops.box_iou(info[\"target_bbox\"],info[\"pred_bbox\"]).item()\n",
    "            iou_count += 1\n",
    "            if iou > 0.5:\n",
    "                accuracy_count += 1\n",
    "            evaluate_iou += iou\n",
    "            print(\"#{: >5} ->BBOX_IOU: {: >10} | mean IOU: {: >10} | accuracy: {: >10}\".format(i,round(iou, 4),round(evaluate_iou/iou_count,4),round(accuracy_count/iou_count,4)))\n",
    "            print(\"Actions: \",actions)\n",
    "            print(\"\\n\\n#######################################################################\")\n",
    "            evaluate_reward += episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: STESSA COSA DI PRIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"Hyperparameter Setting for PPO-discrete\")\n",
    "parser.add_argument(\"-n\", type=int, default=int(-1), help=\" name index\")\n",
    "parser.add_argument(\"--max_train_steps\", type=int, default=int(2e6), help=\" Maximum number of training steps\")\n",
    "parser.add_argument(\"--evaluate_freq\", type=float, default=5e3, help=\"Evaluate the policy every 'evaluate_freq' steps\")\n",
    "parser.add_argument(\"--save_model_freq\", type=int, default=2e4, help=\"Save frequency\")\n",
    "parser.add_argument(\"--evaluate_times\", type=float, default=100, help=\"Evaluate times\")\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1024, help=\"Batch size\")\n",
    "parser.add_argument(\"--mini_batch_size\", type=int, default=128, help=\"Minibatch size\")\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=1024, help=\"The number of neurons in hidden layers of the neural network\")\n",
    "parser.add_argument(\"--lr\", type=float, default=3e-4, help=\"Learning rate of actor\")\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor\")\n",
    "parser.add_argument(\"--lamda\", type=float, default=0.95, help=\"GAE parameter\")\n",
    "parser.add_argument(\"--epsilon\", type=float, default=0.20, help=\"PPO clip parameter\")\n",
    "parser.add_argument(\"--K_epochs\", type=int, default=15, help=\"PPO parameter\")\n",
    "parser.add_argument(\"--use_adv_norm\", type=bool, default=True, help=\"Trick 1:advantage normalization\")\n",
    "parser.add_argument(\"--use_state_norm\", type=bool, default=False, help=\"Trick 2:state normalization\")\n",
    "parser.add_argument(\"--use_reward_scaling\", type=bool, default=True, help=\"Trick 4:reward scaling\")\n",
    "parser.add_argument(\"--entropy_coef\", type=float, default=0.01, help=\"Trick 5: policy entropy\")\n",
    "parser.add_argument(\"--use_lr_decay\", type=bool, default=True, help=\"Trick 6:learning rate Decay\")\n",
    "parser.add_argument(\"--use_grad_clip\", type=bool, default=True, help=\"Trick 7: Gradient clip\")\n",
    "parser.add_argument(\"--use_orthogonal_init\", type=bool, default=True, help=\"Trick 8: orthogonal initialization\")\n",
    "parser.add_argument(\"--set_adam_eps\", type=float, default=True, help=\"Trick 9: set Adam epsilon=1e-5\")\n",
    "parser.add_argument(\"--use_tanh\", type=float, default=True, help=\"Trick 10: tanh activation function\")\n",
    "parser.add_argument(\"--use_gru\", type=bool, default=False, help=\"Whether to use GRU\")\n",
    "parser.add_argument(\"--resume\", type=bool, default=False, help=\"load last weights and resume training from checkpoint\")\n",
    "parser.add_argument(\"--steps_num\", type=int, default=-1, help=\"steps number to load weights\")\n",
    "parser.add_argument(\"--transformer\", type=bool, default=False, help=\"whether to use transformer instead of lstm\")\n",
    "args = parser.parse_args()\n",
    "if args.n == -1:\n",
    "    print(\"Please input the index of the model\")\n",
    "    exit()\n",
    "test = Test(args,args.n)\n",
    "test.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
